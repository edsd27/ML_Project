{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from SystemDegradationEnv import MDPEnvironment\n",
    "import time\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MDPEnvironment(state_size=15, action_size=8)\n",
    "env.save_environment(\"mdp_environment.json\")\n",
    "print(\"✅ Environment gespeichert als 'mdp_environment.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPPOAgent(PPO):\n",
    "    def compute_advantage(self, rewards, values, reference_value):\n",
    "        advantages = []\n",
    "        for t in range(len(rewards)):\n",
    "            A_t = rewards[t] + values[t+1] - reference_value  # Ohne Discounting\n",
    "            advantages.append(A_t)\n",
    "        return np.array(advantages)\n",
    "\n",
    "    def train(self, env, total_timesteps):\n",
    "        obs = env.reset()\n",
    "        for _ in range(total_timesteps):\n",
    "            action, _ = self.predict(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Compute modified advantage\n",
    "            reference_value = self.policy.value_function(env.state_0)  # Referenz-Q-Wert\n",
    "            advantage = self.compute_advantage(reward, self.policy.value_function(obs), reference_value)\n",
    "\n",
    "            # Train with modified loss\n",
    "            self.learn(total_timesteps=1, values=advantage)\n",
    "\n",
    "            obs = next_obs if not done else env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your custom environment\n",
    "env = MDPEnvironment(load_from=\"mdp_environment.json\")\n",
    "\n",
    "# Vectorized Environment (recommended for PPO)\n",
    "vec_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# Create PPO model\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0, learning_rate=3e-4)\n",
    "\n",
    "start_time=time.time()\n",
    "# Train PPO agent\n",
    "model.learn(total_timesteps=10000)\n",
    "end_time = time.time() - start_time\n",
    "print(f\"Dauer des Trainings {end_time - start_time}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_maintenance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(model, env):\n",
    "    policy = {}\n",
    "    for state in range(env.observation_space.n):  # Alle möglichen Zustände durchlaufen\n",
    "        action, _ = model.predict(np.array([state]), deterministic=True)\n",
    "        policy[state] = action  # Beste Aktion für jeden Zustand speichern\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_maintenance\")  # Lade das trainierte Modell\n",
    "policy = extract_policy(model, env)\n",
    "\n",
    "print(\"Optimale Politik:\")\n",
    "for state, action in policy.items():\n",
    "    print(f\"π({state}) = {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"ppo_policy.json\", \"w\") as file:\n",
    "    json.dump(policy, file, indent=4)\n",
    "\n",
    "with open(\"ppo_policy.json\", \"r\") as file:\n",
    "    loaded_policy = json.load(file)\n",
    "print(loaded_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, episodes=100):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = policy.get(state[0], 0)  # Falls Zustand nicht in Politik, wähle Standardaktion\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Durchschnittliche Belohnung mit Politik: {avg_reward:.2f}€\")\n",
    "    return avg_reward\n",
    "\n",
    "evaluate_policy(policy, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
